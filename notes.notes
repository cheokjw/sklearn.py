============================================================
| Features and labels                                      |
============================================================

Features = acts as input (Labelled as X below)
            - One feature does not affect another feature
Labels = acts as output (Labelled as y below)

_____________________________________________________________________
| Feature1  | Feature2 | Feature3 | Feature 4 | Feature 5 | Label   |         
|___________|__________|__________|___________|___________|_________|         
| X1        | X2       | X3       | X4        | X5        | y1      |         ^
|___________|__________|__________|___________|___________|_________|         |
| X6        | X7       | X8       | X9        | X10       | y2      |         |
|___________|__________|__________|___________|___________|_________|     Instances
| X11       | X12      | X13      | X14       | X15       | y3      |         |
|___________|__________|__________|___________|___________|_________|         |
| X16       | X17      | X18      | X19       | X20       | y4      |         |
|___________|__________|__________|___________|___________|_________|         |

<-------------------------   Dimension     -------------------------->

Dimension = 5
Instances = 4
- The number of features are also known as dimensions
- Features are also known as attributes, independent variable, input
- Labels are known as dependent variable, output



============================================================
| Saving a model                                           |
============================================================
 
from sklearn.externals import joblib

# saving
filename = 'model.sav'
joblib.dump(clf, filename)

# open
clf.joblib.load(filename)


============================================================
| Classification   (Supervised Learning)                   |
============================================================
Classfication is a process of categorizing a gicen set of data into classes (Grouping data)
E.g:
    Model had classified three regions (red, blue and green)
    Lets say one of the data point is plotted on the red region. Therefore, the model will classify the data point as region red.

1. KNN (K-nearest neighbours)
Can be used for classification and also regression algorithm
k acts as a constant
For example, when a point is plotted on a graph, the model calculates the disatnce of k-amount of points nearest to the point.
Then, the majority will decide which region the point belongs to

Case scenario:
Lets say k constant = 15
A point is plotted on the graph. Then, the model calculates distance between the 15 nearest point to the point
Then, the closest majority to the point is region blue. Thus, the point is classified as blue



2. SVM (Support Vector Machine)
Can be used for classification and also regression algorithm
Effective for high dimensional spaces (Contains many features)
Many kernel functions

SVM creates a hyperplane (Hyperplane would be a single line in a 2dimension graph and 2dimensional line in a 3dimension graph etc.)
- SVM draws the plane by computing the distance from the support vectors
- The way SVM optimize the model is by creating the largest possible margin (THe higher the margin, the higher the accuracy)
Plane can be linear or sigmoid function (Thus, increasing fault tolerance)

|
|    . .  .   . /    ...
|        .. \  /    ..                                 . = data points/ support vectors
|   ... .    \/   .  .                                 / = hyperplane
|  ..  .     /\    .                                   \ = margin
| .         /  \ .. . 
|   .. .   /   . 
|         /    .. ...
|________________________________

Kernel functions
SVM couldn't seperate the datapoints with plane since they are all clustered together. Therefore, it uses kernel function to seperate it

|                                                 |
|   **********                                    |
|  *   ... .   *              converts            |
| *       .  .  *           ------------>         |     . .. .. . .. 
|  *.... .. .  *                                  |   ***************
|    **********                                   |
|____________________                             |_____________________


=============================================================
| Linear Regression vs Logistic Regression                  |
=============================================================
1. Linear Regression
Linear Regression is a machine learning algorithm based on supervised learning. It performs a regression task. 
Regression models a target prediction value based on features(intdependant variable)
It is mostly used for finding out the relationship between variables and forecasting
Y = mx + c 

R^2 value indicates how accurate the line represents the datapoints 
(higher R^2, higher accuracy)

| 
|    . . /.. 
|    .. /. .                                     . = datapoints
|    . /..
|   . /..
|____________________

2. Logistic Regression
Converts y to logit(y) (sigmoid function)
logit(y) = 1/ (1 + e^y) [never touches 0 and 1]

|                 
|                    . . . .. .
|                  _____________
|              .. /... . .. . .
|                /
|               /
|              /
|             /
| . . .. ..  /
|___________/
| .. .. . .. 
|_________________________________


============================================================
| Clustering (Unsupervised Learning)                       |
============================================================
Clustering is an algorithm where the computer figures out that any point in a classified region is belong to the region

The difference between Clustering and classification is that classification on model trains with labels while clustering models don't

1. K-mean clustering
- It groups similar datapoints together and discover underlying patterns. You'll define a number k, which refers to the number of centroids
Centroids (n number of datapoints, n depends on number of label) = Centroid is an imaginary data point representing the center of the cluster)
Given example where there are 2 features for a dataset.
The algorithm starts by creating 2 centroids. A hyperplane will then be generated between these two centroids,
Then it calculates the average position and changing the position of the plane based on the result
This repeats until a hyperplane that suits the label seperation is found

2D -> 2 features
y = mx + c
y = midpoint ,   x = midpoint  ,    m = normal


3D -> 3 features 
n = (x2- x1, y2, y1, z2, z1)
P = ((x1 + x2)/ 2, (y1 + y2)/ 2, (z1 + z2)/ 2)
(x2 - x1)(x - xp) + (y2 - y1)(y - yp) + (z2 - z1)(z - zp) = 0



===============================================================
| Neural Network                                              |
===============================================================
Refer neural network map image here: https://www.ibm.com/my-en/cloud/learn/neural-networks

Neural netowrk is used to find pattern. It reflects the behavior of the human brain, allowing computer to recognize pattern
- Artificial neural netowrks are compromised of a node layers, containing an input layer, one or more hidden layers, and an output layer. 
- Each node are connected to another and has an associated weight and threshold value. 
- That node is activated, sending data to the next layer of the network. 
 
Linear combination fomula (The way calculating output)
 Σ (xi*wi)+bi 
i=1

Activation function:
    f(x) = y1
When we are training the program, we are inputting alot of data.
The program will adjust its weight in order to optimize the output

Hidden layer:
Allow neural network to figure out more patterns (Adds more complexity into neural network)



=================================================================
| Overfitting vs Underfitting                                   |
=================================================================
1) Overfitting 
Overfitting refers to a model that models the training data too well
- It happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data
- This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model 
- The problem is that these concepts do not apply to new data and negatively impact the models ability to generalize.

Example:
    Say you are visiting a foreign country and the taxi driver rips you off. You might be
    tempted to say that all taxi drivers in that country are thieves. Overgeneralizing is
    something that we humans do all too often,

Can be fixed by:
    • To simplify the model by selecting one with fewer parameters
    (e.g., a linear model rather than a high-degree polynomial
    model), by reducing the number of attributes in the training
    data or by constraining the model
    • To gather more training data
    • To reduce the noise in the training data (e.g., fix data errors
    and remove outliers)


2) Underfitting 
Underfitting refers to a model that can neither model the training data nor generalize to new data


==================================================================
| Loss Function & Gradient Descents                              |
==================================================================
1) Loss Function/ Cost function 
Objective function is used to evaluate a candidate solution(e.g: a set of weights)
We may seek to maximuze or minimize the objective function, meaning that we are searching for a candidate solution that has the highest or lowest score respectively.
Goal: Typically, with neural networks, we seek to minimize the error.

       n     
1/n *  Σ(y - ȳ)^2
       i

It tells how bad the output the neural network produced that allows it to adjust its weights and biases in order to produce the correct output

2) Gradient Descent
Gradient Descent is the way that neural network used to minimize the cost function 
To do so, we took the gradient of the cost function and they will give us a vector of the weights (w1, w2, w3 .... etc)
Taking the negative of the gradient vector would give us the direction or how much we should change these weight in order to minimize the cost


=====================================================================
| Backpropogation                                                   |
=====================================================================
Backpropogation is the algorithm that is used on one training sets in order to calculate these weights (or the change we want to apply to a certain weights)
input         output
o   
o    -> o 
o    -> o    -> o 
o    -> o 
o 
(neural network map)
-----------------> = forward propogation
<----------------- = backward propogation

Backward propogation is basically taking the information from the output layer and going backwards, taking the error and calculating backwards to get to the error on certain weights
The errors are composition function of all of the activation function of hidden layers before it
E.g:-
dE/dw = dE/da * da/dZ * dZ/dw
